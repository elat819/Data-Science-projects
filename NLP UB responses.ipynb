{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1612\n",
      "Maximum length: 2683\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 2683, 100)         161200    \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 2664, 32)          64032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1332, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 42624)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                426250    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 651,493\n",
      "Trainable params: 651,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      " - 3s - loss: 0.7102 - accuracy: 0.4000\n",
      "Epoch 2/15\n",
      " - 2s - loss: 0.6715 - accuracy: 0.5714\n",
      "Epoch 3/15\n",
      " - 2s - loss: 0.6682 - accuracy: 0.6571\n",
      "Epoch 4/15\n",
      " - 2s - loss: 0.6544 - accuracy: 0.6286\n",
      "Epoch 5/15\n",
      " - 2s - loss: 0.6403 - accuracy: 0.6000\n",
      "Epoch 6/15\n",
      " - 2s - loss: 0.6285 - accuracy: 0.6000\n",
      "Epoch 7/15\n",
      " - 2s - loss: 0.6086 - accuracy: 0.6286\n",
      "Epoch 8/15\n",
      " - 2s - loss: 0.6307 - accuracy: 0.6286\n",
      "Epoch 9/15\n",
      " - 2s - loss: 0.6491 - accuracy: 0.4857\n",
      "Epoch 10/15\n",
      " - 2s - loss: 0.5761 - accuracy: 0.7429\n",
      "Epoch 11/15\n",
      " - 2s - loss: 0.6591 - accuracy: 0.6000\n",
      "Epoch 12/15\n",
      " - 2s - loss: 0.6085 - accuracy: 0.6000\n",
      "Epoch 13/15\n",
      " - 2s - loss: 0.5494 - accuracy: 0.6286\n",
      "Epoch 14/15\n",
      " - 2s - loss: 0.5407 - accuracy: 0.8000\n",
      "Epoch 15/15\n",
      " - 2s - loss: 0.5323 - accuracy: 0.9143\n",
      "Vocabulary size: 1612\n",
      "Maximum length: 2683\n",
      "Train Accuracy: 91.43\n",
      "Test Accuracy: 61.62\n",
      "Review: [I really liked the unconscious Bias material and exercises.  It was a great class and fantastic. I would recommend]\n",
      "Sentiment: POSITIVE (50.552%)\n",
      "Review: [I felt very uncomfortable. It was very glitchy and too long.]\n",
      "Sentiment: NEGATIVE (51.210%)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.models import load_model\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    \n",
    "    file = open(filename, errors=\"ignore\")\n",
    "    #file = open(filename, 'r')\n",
    "    \n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "    \n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "        \n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        \n",
    "        \n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('p0'):  ##Check filename\n",
    "            continue\n",
    "            \n",
    "        if not is_train and filename.startswith('trn'):  #check filename\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "               \n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        \n",
    "        \"\"\"\n",
    "        if directory is 'neg':\n",
    "            #os.chdir('C:/Users/ejl9900/Desktop/Data Science/MSDS453/NLP Project - Survey Results/neg')\n",
    "            #doc = pd.read_csv('Survey Results Negative.csv', encoding='cp1252')\n",
    "            doc = load_doc ('C:/Users/ejl9900/Desktop/Data Science/MSDS453/NLP Project - Survey Results/neg/Survey Results Negative - Copy.csv')\n",
    "        \n",
    "        if directory is 'pos':\n",
    "            #os.chdir('C:/Users/ejl9900/Desktop/Data Science/MSDS453/NLP Project - Survey Results/pos')\n",
    "            #doc = pd.read_csv('Survey Results Positive.csv', encoding='cp1252')\n",
    "            doc = load_doc ('C:/Users/ejl9900/Desktop/Data Science/MSDS453/NLP Project - Survey Results/pos/Survey Results Positive v2.csv')\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "  \n",
    "    return documents\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    #neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "    #pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "    neg = process_docs('neg', vocab, is_train)\n",
    "    pos = process_docs('pos', vocab, is_train)\n",
    "    \n",
    "    \n",
    "    docs = neg + pos\n",
    "\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    \n",
    "    #print(labels)\n",
    "    \n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    #model.add(Conv1D(32, 8, activation='relu'))  #8\n",
    "    model.add(Conv1D(32, 20, activation='relu'))  #8\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))  #10\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  #optimizer='adam'\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model_survey.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab_survey.txt'\n",
    "os.chdir('C:/Users/ejl9900/Desktop/Data Science/MSDS453/NLP Project - Survey Results')\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "\n",
    "#print(train_docs)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "#15.5. Evaluate Model 167\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])  #######\n",
    "print('Maximum length: %d' % max_length)\n",
    "\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "\n",
    "# define model\n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=15, verbose=2)\n",
    "\n",
    "# save the model\n",
    "model.save('model_survey.h5')\n",
    "\n",
    "\n",
    "\n",
    "###Evaluation Model###\n",
    "\n",
    "\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    # clean review\n",
    "    line = clean_doc(review, vocab)\n",
    "    # encode and pad review\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab_survey.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "\n",
    "\n",
    "# load the model\n",
    "model = load_model('model_survey.h5')\n",
    "\n",
    "# evaluate model on training dataset\n",
    "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %.2f' % (acc*100))\n",
    "\n",
    "\n",
    "# test positive text\n",
    "#text1 = 'I love this class and recommend it. The discussion was very valuable and I felt very comfortable.  The instructor did a great job and I love this training'\n",
    "#text1 = 'This training was great. Cindi was fantastic.  I learned alot.'\n",
    "text1 = 'I really liked the unconscious Bias material and exercises.  It was a great class and fantastic. I would recommend'\n",
    "percent, sentiment = predict_sentiment(text1, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text1, sentiment, percent*100))\n",
    "\n",
    "# test negative text\n",
    "#text2 = 'This was a waste of time. It was horrible and boring.  I felt very uncomfortable.  The training was too long.'\n",
    "#text2 = 'I felt very uncomfortable. It was too long and horrible. The examples are overused, boring, and were poorly incorporated into the training.'\n",
    "text2 = 'I felt very uncomfortable. It was very glitchy and too long.'\n",
    "percent, sentiment = predict_sentiment(text2, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text2, sentiment, percent*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
